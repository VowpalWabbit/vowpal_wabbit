Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
creating cache_file = train-sets/er_small.vw.cache
Reading datafile = train-sets/er_small.vw
num sources = 1
Enabled reductions: gd, scorer-identity, csoaa, search_task
average    since      instance            current true      current predicted   cur   cur   predic    cache  examples          
loss       last        counter           output prefix          output prefix  pass   pol     made     hits    gener  beta    
1.000000   1.000000          1  [4                   ] [1                   ]     0     0        1        0        1  0.000000
2.500000   4.000000          2  [2 4 2 5 10 10       ] [4 4 4 7 7 7         ]     0     0        7        0        7  0.000000
3.250000   4.000000          4  [1 4 4 1 10 10 10 1..] [4 4 4 4 10 10 10 1..]     0     0       32        0       32  0.000000
1.625000   0.000000          8  [1 4 4 1 10 10 10 1..] [1 4 4 1 10 10 10 1..]     1     0       42        0       64  0.000001
0.812500   0.000000         16  [1 4 4 1 10 10 10 1..] [1 4 4 1 10 10 10 1..]     3     0       52        0      128  0.000001

finished run
number of examples per pass = 4
passes used = 4
weighted example sum = 16.000000
weighted label sum = 0.000000
average loss = undefined (no holdout)
total feature number = 649
