using no cache
Reading datafile = train-sets/regression_small.txt
num sources = 1
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
Enabled learners: gd, scorer-identity, binary, cats_tree, get_pmf, pmf_to_pdf, cb_explore_pdf, cats_pdf, sample_pdf, cats, cbify-reg, count_label
Input label = SIMPLE
Output pred = SCALAR
average  since         example        example        current        current  current
loss     last          counter         weight          label        predict features
0.446502 0.446502            1            1.0         1.0000         0.1070        2
0.362893 0.279285            2            2.0         1.0000         0.4414        2
0.473996 0.585099            4            4.0         1.0000         0.1654        2
0.401201 0.328405            8            8.0         1.0000         1.1260        2
0.313404 0.225607           16           16.0         1.0000         1.7194        2
0.264559 0.215714           32           32.0         1.0000         0.9713        2
0.288815 0.313072           64           64.0         1.0000         1.7331        2

finished run
number of examples = 100
weighted example sum = 100.000000
weighted label sum = 120.258546
average loss = 0.302421
best constant = 1.202585
best constant's loss = 0.120504
total feature number = 233
Learn() count per node: id=0, #l=18; id=1, #l=19; id=2, #l=23; id=3, #l=11; id=4, #l=30; id=5, #l=19; id=6, #l=40; 
