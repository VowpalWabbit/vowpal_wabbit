final_regressor = models/freegrad.model
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = train-sets/0001.dat.cache
ignoring text input in favor of cache input
num sources = 1
Enabled reductions: gd, scorer-identity, count_label
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0   1.0000   0.0000       51
0.513615 0.027230            2            2.0   0.0000   0.1650      104
0.263120 0.012625            4            4.0   0.0000   0.0569      135
0.237735 0.212350            8            8.0   0.0000   0.2024      146
0.248566 0.259397           16           16.0   1.0000   0.2048      143
0.230775 0.212983           32           32.0   1.0000   0.4685       70
0.232953 0.235132           64           64.0   0.0000   0.4225       34
0.219768 0.206583          128          128.0   0.0000   0.1011       30
0.164102 0.164102          256          256.0   0.0000   0.1327       72 h
0.174173 0.184245          512          512.0   0.0000   0.0000       37 h

finished run
number of examples per pass = 180
passes used = 4
weighted example sum = 720.000000
weighted label sum = 320.000000
average loss = 0.153097 h
best constant = 0.444444
best constant's loss = 0.246914
total feature number = 55132
