Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
creating cache_file = train-sets/big-constant.dat.cache
Reading datafile = train-sets/big-constant.dat
num sources = 1
Enabled reductions: gd, scorer-identity
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.102561 0.102561            1            1.0 1000.3203 1000.0000       23
0.357076 0.611592            2            2.0 999.5382 1000.3203       23
3.682311 7.007545            4            4.0 1001.4025 998.1735       23
4.046612 4.410912            8            8.0 1000.9435 999.5549       23
3.395124 2.743636           16           16.0 1003.0876 1000.4109       23
2.562042 1.728961           32           32.0 1000.0400 999.6088       23
1.986634 1.411226           64           64.0 1001.1793 1000.0135       23
1.758225 1.529815          128          128.0 998.8785 999.7435       23
1.330741 0.903257          256          256.0 1000.6226 1000.4050       23
0.934198 0.537654          512          512.0 1001.3119 1000.6470       23
0.586292 0.238387         1024         1024.0 1001.0089 1000.9322       23
0.332234 0.078176         2048         2048.0 999.2149 999.4188       23
0.174342 0.016450         4096         4096.0 999.5160 999.5194       23
0.088086 0.001830         8192         8192.0 999.5843 999.5402       23

finished run
number of examples per pass = 100
passes used = 100
weighted example sum = 10000.000000
weighted label sum = 9998960.528564
average loss = 0.072191
best constant = 999.896118
total feature number = 230000
