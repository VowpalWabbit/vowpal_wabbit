creating cache_file = train-sets/big-constant.dat.cache
Reading datafile = train-sets/big-constant.dat
num sources = 1
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
Enabled reductions: gd, scorer-identity, count_label
Input label = simple
Output pred = scalar
average  since         example        example        current        current  current 
loss     last          counter         weight          label        predict features 
0.102561 0.102561            1            1.0      1000.3203      1000.0000       23 
0.357076 0.611592            2            2.0       999.5382      1000.3203       23 
3.682409 7.007742            4            4.0      1001.4025       998.1735       23 
4.046689 4.410968            8            8.0      1000.9435       999.5549       23 
3.395164 2.743639           16           16.0      1003.0876      1000.4110       23 
2.562052 1.728941           32           32.0      1000.0400       999.6088       23 
1.986638 1.411223           64           64.0      1001.1793      1000.0135       23 
1.758230 1.529822          128          128.0       998.8785       999.7435       23 
1.330744 0.903258          256          256.0      1000.6226      1000.4048       23 
0.934204 0.537664          512          512.0      1001.3119      1000.6471       23 
0.586297 0.238391         1024         1024.0      1001.0089      1000.9322       23 
0.332237 0.078176         2048         2048.0       999.2149       999.4188       23 
0.174343 0.016450         4096         4096.0       999.5160       999.5194       23 
0.088087 0.001830         8192         8192.0       999.5843       999.5402       23 

finished run
number of examples per pass = 100
passes used = 100
weighted example sum = 10000.000000
weighted label sum = 9998960.528564
average loss = 0.072192
best constant = 999.896118
total feature number = 230000
