creating cache_file = train-sets/big-constant.dat.cache
Reading datafile = train-sets/big-constant.dat
num sources = 1
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
Enabled learners: gd, scorer-identity, count_label
Input label = SIMPLE
Output pred = SCALAR
average  since         example        example        current        current  current
loss     last          counter         weight          label        predict features
0.102561 0.102561            1            1.0      1000.3203      1000.0000       23
0.357076 0.611592            2            2.0       999.5382      1000.3203       23
3.682311 7.007545            4            4.0      1001.4025       998.1735       23
4.046612 4.410912            8            8.0      1000.9435       999.5549       23
3.395124 2.743636           16           16.0      1003.0876      1000.4109       23
2.562042 1.728961           32           32.0      1000.0400       999.6088       23
1.986634 1.411226           64           64.0      1001.1793      1000.0135       23
1.758225 1.529815          128          128.0       998.8785       999.7435       23
1.330741 0.903257          256          256.0      1000.6226      1000.4050       23
0.934198 0.537654          512          512.0      1001.3119      1000.6470       23
0.586292 0.238387         1024         1024.0      1001.0089      1000.9322       23
0.332234 0.078176         2048         2048.0       999.2149       999.4188       23
0.174342 0.016450         4096         4096.0       999.5160       999.5194       23
0.088086 0.001830         8192         8192.0       999.5843       999.5402       23

finished run
number of examples per pass = 100
passes used = 100
weighted example sum = 10000.000000
weighted label sum = 9998960.528564
average loss = 0.072191
best constant = 999.896118
total feature number = 230000
