using l1 regularization = 0.01
final_regressor = models/mask.model
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = train-sets/0001.dat
num sources = 1
Enabled reductions: gd, scorer-identity
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0   1.0000   0.0000       51
0.513258 0.026516            2            2.0   0.0000   0.1628      104
0.262827 0.012396            4            4.0   0.0000   0.0559      135
0.237630 0.212433            8            8.0   0.0000   0.2043      146
0.243325 0.249019           16           16.0   1.0000   0.3164       24
0.237450 0.231576           32           32.0   0.0000   0.2108       32
0.233099 0.228747           64           64.0   0.0000   0.2045       61
0.233283 0.233467          128          128.0   1.0000   0.4665      106

finished run
number of examples = 200
weighted example sum = 200.000000
weighted label sum = 91.000000
average loss = 0.224127
best constant = 0.455000
best constant's loss = 0.247975
total feature number = 15482
