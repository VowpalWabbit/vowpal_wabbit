Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
creating cache_file = train-sets/argmax_data.cache
Reading datafile = train-sets/argmax_data
num sources = 1
Enabled reductions: gd, scorer-identity, csoaa, search_task
average    since      instance            current true      current predicted   cur   cur   predic    cache  examples          
loss       last        counter           output prefix          output prefix  pass   pol     made     hits    gener  beta    
10.000000  10.000000         1  [2                   ] [1                   ]     0     0        5        0        5  0.000000
5.500000   1.000000          2  [1                   ] [2                   ]     0     0        9        0        9  0.000000
5.250000   5.000000          4  [2                   ] [1                   ]     0     0       15        0       15  0.000000
2.875000   0.500000          8  [2                   ] [2                   ]     1     0       30        0       30  0.000000
1.687500   0.500000         16  [2                   ] [2                   ]     3     0       60        0       60  0.000001
1.093750   0.500000         32  [2                   ] [2                   ]     7     0      120        0      120  0.000001
0.796875   0.500000         64  [2                   ] [2                   ]    15     0      240        0      240  0.000002

finished run
number of examples per pass = 4
passes used = 20
weighted example sum = 80.000000
weighted label sum = 0.000000
average loss = 0.737500
total feature number = 900
