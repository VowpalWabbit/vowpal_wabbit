creating cubic features for triples: abc 
final_regressor = models/xxor.model
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
creating cache_file = train-sets/xxor.dat.cache
Reading datafile = train-sets/xxor.dat
num sources = 1
Enabled reductions: gd, scorer-identity
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0   1.0000   0.0000        5
0.581525 0.163049            2            2.0   0.0000   0.4038        5
0.422704 0.105064            3            3.0   0.0000   0.3241        5
0.567028 1.000000            4            4.0   1.0000   0.0000        5
0.527970 0.449852            6            6.0   1.0000   0.1235        5
0.493858 0.391522            8            8.0   0.0000   0.6019        5
0.407022 0.175458           11           11.0   0.0000   0.4429        5
0.361109 0.234848           15           15.0   1.0000   0.5645        5
0.304274 0.133770           20           20.0   1.0000   0.5956        5
0.249203 0.091857           27           27.0   0.0000   0.2644        5
0.201723 0.059283           36           36.0   1.0000   0.7710        5
0.158986 0.030775           48           48.0   0.0000   0.1478        5
0.122699 0.013839           64           64.0   0.0000   0.0883        5
0.092576 0.004943           86           86.0   1.0000   0.9496        5
0.069532 0.001197          115          115.0   0.0000   0.0223        5
0.051971 0.000188          154          154.0   0.0000   0.0084        5
0.038856 0.000016          206          206.0   1.0000   0.9988        5
0.029107 0.000001          275          275.0   0.0000   0.0002        5
0.021811 0.000000          367          367.0   1.0000   1.0000        5
0.016336 0.000000          490          490.0   0.0000   0.0000        5
0.012239 0.000000          654          654.0   1.0000   1.0000        5

finished run
number of examples per pass = 8
passes used = 100
weighted example sum = 800.000000
weighted label sum = 400.000000
average loss = 0.010006
best constant = 0.500000
best constant's loss = 0.250000
total feature number = 4000
