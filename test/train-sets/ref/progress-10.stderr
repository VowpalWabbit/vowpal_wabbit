Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = train-sets/0001.dat
num sources = 1
Enabled reductions: gd, scorer-identity
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.251938 0.251938           10           10.0   1.0000   0.2253       34
0.235272 0.218607           20           20.0   0.0000   0.2122      104
0.240930 0.252246           30           30.0   0.0000   0.3390       82
0.229258 0.194240           40           40.0   1.0000   0.5489       42
0.225418 0.210062           50           50.0   0.0000   0.2070       60
0.232323 0.266844           60           60.0   0.0000   0.3381      147
0.229975 0.215893           70           70.0   1.0000   0.4733      134
0.226344 0.200927           80           80.0   0.0000   0.2216      136
0.217409 0.145923           90           90.0   0.0000   0.2602      139
0.216703 0.210351          100          100.0   1.0000   0.3171       56
0.218356 0.234887          110          110.0   1.0000   0.5796       97
0.226824 0.319970          120          120.0   0.0000   0.4137      120
0.223091 0.178299          130          130.0   1.0000   0.4105       54
0.218306 0.156103          140          140.0   0.0000   0.3486       82
0.212760 0.135116          150          150.0   1.0000   0.4022      148
0.211695 0.195720          160          160.0   0.0000   0.6071       63
0.206603 0.125119          170          170.0   1.0000   0.7091       69
0.202214 0.127603          180          180.0   1.0000   0.7582       42
0.198753 0.136457          190          190.0   1.0000   0.7647       34
0.195760 0.138897          200          200.0   1.0000   0.5243       56

finished run
number of examples = 200
weighted example sum = 200.000000
weighted label sum = 91.000000
average loss = 0.195760
best constant = 0.455000
best constant's loss = 0.247975
total feature number = 15482
