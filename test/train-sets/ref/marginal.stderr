final_regressor = marginal_model
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
creating cache_file = train-sets/marginal_features.cache
Reading datafile = train-sets/marginal_features
num sources = 1
Enabled reductions: gd, marginal, scorer-identity
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.250000 0.250000            1            1.0   0.5000   0.0000        4
0.318194 0.386387            2            2.0   1.0000   0.3784        4
0.221573 0.124952            4            4.0   0.5000   0.4221        4
0.140273 0.058974            8            8.0   0.5000   0.4000        4
0.086155 0.032037           16           16.0   0.5000   0.4597        4
0.052647 0.019140           32           32.0   0.5000   0.4706        4
0.031218 0.009789           64           64.0   0.5000   0.4799        4
0.017241 0.003264          128          128.0   0.5000   0.4913        4
0.008857 0.000473          256          256.0   0.5000   0.4995        4

finished run
number of examples per pass = 4
passes used = 100
weighted example sum = 400.000000
weighted label sum = 225.000000
average loss = 0.005678
best constant = 0.562500
total feature number = 1600
