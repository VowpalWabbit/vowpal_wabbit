Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = train-sets/0001.dat
num sources = 1
Enabled reductions: gd, nn, scorer-identity
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0   1.0000   0.0000       51
0.583598 0.167197            2            2.0   0.0000   0.4089      104
0.346844 0.110089            4            4.0   0.0000   0.3112      135
0.273247 0.199650            8            8.0   0.0000   0.2907      146
0.261045 0.248843           16           16.0   1.0000   0.2922       24
0.251549 0.242054           32           32.0   0.0000   0.3430       32
0.244875 0.238201           64           64.0   0.0000   0.3535       61
0.243892 0.242908          128          128.0   1.0000   0.5382      106

finished run
number of examples = 200
weighted example sum = 200.000000
weighted label sum = 91.000000
average loss = 0.230748
best constant = 0.455000
best constant's loss = 0.247975
total feature number = 15482
