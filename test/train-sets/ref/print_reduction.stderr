using no cache
Reading datafile = train-sets/regression_small.txt
num sources = 1
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
Enabled learners: print, scorer-identity, count_label
Input label = SIMPLE
Output pred = SCALAR
average  since         example        example        current        current  current
loss     last          counter         weight          label        predict features
1 | 259761 11650396 
1 | 259761 11650396 
1.000000 1.000000            1            1.0         1.0000         0.0000        2
1 | 82936 11650396 
1 | 82936 11650396 
1.000000 1.000000            2            2.0         1.0000         0.0000        2
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1.750000 2.500000            4            4.0         1.0000         0.0000        2
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
1.750000 1.750000            8            8.0         1.0000         0.0000        2
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1.937500 2.125000           16           16.0         1.0000         0.0000        2
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
1.937500 1.937500           32           32.0         1.0000         0.0000        2
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1.984375 2.031250           64           64.0         1.0000         0.0000        2
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 
1 | 82936 11650396 
1 | 82936 11650396 
2 | 259761 82936 11650396 
2 | 259761 82936 11650396 
1 | 259761 11650396 
1 | 259761 11650396 

finished run
number of examples = 100
weighted example sum = 100.000000
weighted label sum = 133.000000
average loss = 1.990000
best constant = 1.330000
best constant's loss = 0.221100
total feature number = 233
