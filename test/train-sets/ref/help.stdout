driver:
  --onethread           Disable parse thread
VW options:
  --ring_size arg (=256, ) size of example ring
  --strict_parse           throw on malformed examples
Update options:
  -l [ --learning_rate ] arg Set learning rate
  --power_t arg              t power value
  --decay_learning_rate arg  Set Decay factor for learning_rate between passes
  --initial_t arg            initial t value
  --feature_mask arg         Use existing regressor to determine which 
                             parameters may be updated.  If no 
                             initial_regressor given, also used for initial 
                             weights.
Weight options:
  -i [ --initial_regressor ] arg  Initial regressor(s)
  --initial_weight arg            Set all weights to an initial value of arg.
  --random_weights                make initial weights random
  --normal_weights                make initial weights normal
  --truncated_normal_weights      make initial weights truncated normal
  --sparse_weights                Use a sparse datastructure for weights
  --input_feature_regularizer arg Per feature regularization input file
Parallelization options:
  --span_server arg                 Location of server for setting up spanning 
                                    tree
  --unique_id arg (=0, )            unique id used for cluster parallel jobs
  --total arg (=1, )                total number of nodes used in cluster 
                                    parallel job
  --node arg (=0, )                 node number in cluster parallel job
  --span_server_port arg (=26543, ) Port of the server for setting up spanning 
                                    tree
Diagnostic options:
  --version             Version information
  -a [ --audit ]        print weights of features
  -P [ --progress ] arg Progress update frequency. int: additive, float: 
                        multiplicative
  --quiet               Don't output diagnostics and progress updates
  --limit_output arg    Avoid chatty output. Limit total printed lines.
  --dry_run             Parse arguments and print corresponding metadata. Will 
                        not execute driver.
  -h [ --help ]         More information on vowpal wabbit can be found here 
                        https://vowpalwabbit.org.
Randomization options:
  --random_seed arg     seed random number generator
Feature options:
  --privacy_activation                  turns on aggregated weight exporting when 
                                        the unique feature tags cross 
                                        `privacy_activation_threshold`
  --privacy_activation_threshold arg    takes effect when `privacy_activation` is 
                                        turned on and is the number of unique tag 
                                        hashes a weight needs to see before it is 
                                        exported
  --hash arg                            how to hash the features. Available 
                                        options: strings, all
  --hash_seed arg (=0, )                seed for hash function
  --ignore arg                          ignore namespaces beginning with 
                                        character <arg>
  --ignore_linear arg                   ignore namespaces beginning with 
                                        character <arg> for linear terms only
  --keep arg                            keep namespaces beginning with 
                                        character <arg>
  --redefine arg                        redefine namespaces beginning with 
                                        characters of std::string S as 
                                        namespace N. <arg> shall be in form 
                                        'N:=S' where := is operator. Empty N or
                                        S are treated as default namespace. Use
                                        ':' as a wildcard in S.
  -b [ --bit_precision ] arg            number of bits in the feature table
  --noconstant                          Don't add a constant feature
  -C [ --constant ] arg                 Set initial value of constant
  --ngram arg                           Generate N grams. To generate N grams 
                                        for a single namespace 'foo', arg 
                                        should be fN.
  --skips arg                           Generate skips in N grams. This in 
                                        conjunction with the ngram tag can be 
                                        used to generate generalized 
                                        n-skip-k-gram. To generate n-skips for 
                                        a single namespace 'foo', arg should be
                                        fN.
  --feature_limit arg                   limit to N unique features per 
                                        namespace. To apply to a single 
                                        namespace 'foo', arg should be fN
  --affix arg                           generate prefixes/suffixes of features;
                                        argument '+2a,-3b,+1' means generate 
                                        2-char prefixes for namespace a, 3-char
                                        suffixes for b and 1 char prefixes for 
                                        default namespace
  --spelling arg                        compute spelling features for a give 
                                        namespace (use '_' for default 
                                        namespace)
  --dictionary arg                      read a dictionary for additional 
                                        features (arg either 'x:file' or just 
                                        'file')
  --dictionary_path arg                 look in this directory for 
                                        dictionaries; defaults to current 
                                        directory or env{PATH}
  --interactions arg                    Create feature interactions of any 
                                        level between namespaces.
  --experimental_full_name_interactions arg
                                        EXPERIMENTAL: Create feature 
                                        interactions of any level between 
                                        namespaces by specifying the full name 
                                        of each namespace.
  --permutations                        Use permutations instead of 
                                        combinations for feature interactions 
                                        of same namespace.
  --leave_duplicate_interactions        Don't remove interactions with 
                                        duplicate combinations of namespaces. 
                                        For ex. this is a duplicate: '-q ab -q 
                                        ba' and a lot more in '-q ::'.
  -q [ --quadratic ] arg                Create and use quadratic features
  --q: arg                              DEPRECATED ':' corresponds to a 
                                        wildcard for all printable characters
  --cubic arg                           Create and use cubic features
Example options:
  -t [ --testonly ]                Ignore label information and just test
  --holdout_off                    no holdout data in multiple passes
  --holdout_period arg (=10, )     holdout period for test only
  --holdout_after arg              holdout after n training examples, default 
                                   off (disables holdout_period)
  --early_terminate arg (=3, )     Specify the number of passes tolerated when 
                                   holdout loss doesn't decrease before early 
                                   termination
  --passes arg                     Number of Training Passes
  --initial_pass_length arg        initial number of examples per pass
  --examples arg                   number of examples to parse
  --min_prediction arg             Smallest prediction to output
  --max_prediction arg             Largest prediction to output
  --sort_features                  turn this on to disregard order in which 
                                   features have been defined. This will lead 
                                   to smaller cache sizes
  --loss_function arg (=squared, ) Specify the loss function to be used, uses 
                                   squared by default. Currently available ones
                                   are squared, classic, hinge, logistic, 
                                   quantile and poisson.
  --quantile_tau arg (=0.5, )      Parameter \tau associated with Quantile 
                                   loss. Defaults to 0.5
  --l1 arg                         l_1 lambda
  --l2 arg                         l_2 lambda
  --no_bias_regularization         no bias in regularization
  --named_labels arg               use names for labels (multiclass, etc.) 
                                   rather than integers, argument specified all
                                   possible labels, comma-sep, eg 
                                   "--named_labels Noun,Verb,Adj,Punc"
Output model:
  -f [ --final_regressor ] arg          Final regressor
  --readable_model arg                  Output human-readable final regressor 
                                        with numeric features
  --invert_hash arg                     Output human-readable final regressor 
                                        with feature names.  Computationally 
                                        expensive.
  --save_resume                         save extra state so learning can be 
                                        resumed later with new data
  --preserve_performance_counters       reset performance counters when 
                                        warmstarting
  --save_per_pass                       Save the model after every pass over 
                                        data
  --output_feature_regularizer_binary arg
                                        Per feature regularization output file
  --output_feature_regularizer_text arg Per feature regularization output file,
                                        in text
  --id arg                              User supplied ID embedded into the 
                                        final regressor
Output options:
  -p [ --predictions ] arg     File to output predictions to
  -r [ --raw_predictions ] arg File to output unnormalized predictions to
Input options:
  -d [ --data ] arg     Example set
  --daemon              persistent daemon mode on port 26542
  --foreground          in persistent daemon mode, do not run in the background
  --port arg            port to listen on; use 0 to pick unused port
  --num_children arg    number of children for persistent daemon mode
  --pid_file arg        Write pid file in persistent daemon mode
  --port_file arg       Write port used in persistent daemon mode
  -c [ --cache ]        Use a cache.  The default is <data>.cache
  --cache_file arg      The location(s) of cache_file.
  --json                Enable JSON parsing.
  --dsjson              Enable Decision Service JSON parsing.
  -k [ --kill_cache ]   do not reuse existing cache: create a new one always
  --compressed          use gzip format whenever possible. If a cache file is 
                        being created, this option creates a compressed cache 
                        file. A mixture of raw-text & compressed inputs are 
                        supported with autodetection.
  --no_stdin            do not default to reading from stdin
  --no_daemon           Force a loaded daemon or active learning model to 
                        accept local input instead of starting in daemon mode
  --chain_hash          Enable chain hash in JSON for feature name and string 
                        feature value. e.g. {'A': {'B': 'C'}} is hashed as 
                        A^B^C. Note: this will become the default in a future 
                        version, so enabling this option will migrate you to 
                        the new behavior and silence the warning.
  --flatbuffer          data file will be interpreted as a flatbuffer file
OjaNewton options:
  --OjaNewton                    Online Newton with Oja's Sketch
  --sketch_size arg (=10, )      size of sketch
  --epoch_size arg (=1, )        size of epoch
  --alpha arg (=1, )             mutiplicative constant for indentiy
  --alpha_inverse arg            one over alpha, similar to learning rate
  --learning_rate_cnt arg (=2, ) constant for the learning rate 1/t
  --normalize arg                normalize the features or not
  --random_init arg              randomize initialization of Oja or not
Active Learning:
  --active                enable active learning
  --simulation            active learning simulation mode
  --mellowness arg (=8, ) active learning mellowness parameter c_0. Default 8
Active Learning with Cover:
  --active_cover                enable active learning with cover
  --mellowness arg (=8, )       active learning mellowness parameter c_0. 
                                Default 8.
  --alpha arg (=1, )            active learning variance upper bound parameter 
                                alpha. Default 1.
  --beta_scale arg (=3.16228, ) active learning variance upper bound parameter 
                                beta_scale. Default std::sqrt(10).
  --cover arg (=12, )           cover size. Default 12.
  --oracular                    Use Oracular-CAL style query or not. Default 
                                false.
Audit Regressor:
  --audit_regressor arg stores feature names and their regressor values. Same 
                        dataset must be used for both regressor training and 
                        this mode.
Autolink:
  --autolink arg        create link function with polynomial d
Debug: automl reduction:
  --automl arg (=3, )                set number of live configs
  --global_lease arg (=10, )         set initial lease for automl interactions
  --cm_type arg (=interaction, )     set type of config manager
  --priority_type arg (=none, )      set function to determine next config 
                                     {none, least_exclusion}
  --priority_challengers arg (=-1, ) set number of priority challengers to use
Baseline options:
  --baseline            Learn an additive baseline (from constant features) and
                        a residual separately in regression.
  --lr_multiplier arg   learning rate multiplier for baseline model
  --global_only         use separate example with only global constant for 
                        baseline predictions
  --check_enabled       only use baseline when the example contains enabled 
                        flag
Baseline challenger reduction: Build a CI around the baseline action and use it instead of the model if it's perfoming better:
  --baseline_challenger_cb   Enable reduction
  --cb_c_alpha arg (=0.05, ) Confidence level for 
  --cb_c_tau arg (=0.999, )  Time constant for count decay
Conjugate Gradient options:
  --conjugate_gradient  use conjugate gradient based optimization
LBFGS and Conjugate Gradient options:
  --bfgs                       use conjugate gradient based optimization
  --hessian_on                 use second derivative in line search
  --mem arg (=15, )            memory in bfgs
  --termination arg (=0.001, ) Termination threshold
Binary loss:
  --binary              report loss as binary classification on -1,1
Boosting:
  --boosting arg        Online boosting with <N> weak learners
  --gamma arg (=0.1, )  weak learner's edge (=0.1), used only by online BBM
  --alg arg (=BBM, )    specify the boosting algorithm: BBM (default), logistic
                        (AdaBoost.OL.W), adaptive (AdaBoost.OL)
Bootstrap:
  --bootstrap arg       k-way bootstrap by online importance resampling
  --bs_type arg         prediction type {mean,vote}
Continuous actions tree with smoothing:
  --cats arg            number of discrete actions <k> for cats
  --min_value arg       Minimum continuous value
  --max_value arg       Maximum continuous value
  --bandwidth arg       Bandwidth (radius) of randomization around discrete 
                        actions in terms of continuous range. By default will 
                        be set to half of the continuous action unit-range 
                        resulting in smoothing that stays inside the action 
                        space unit-range:
                        unit_range = (max_value - min_value)/num-of-actions
                        default bandwidth = unit_range / 2.0
Continuous action tree with smoothing with full pdf:
  --cats_pdf arg        number of tree labels <k> for cats_pdf
CATS Tree Options:
  --cats_tree arg             CATS Tree with <k> labels
  --tree_bandwidth arg (=0, ) tree bandwidth for continuous actions in terms of
                              #actions
  --link arg                  Specify the link function: identity, logistic, 
                              glf1 or poisson
Contextual Bandit Options:
  --cb arg              Use contextual bandit learning with <k> costs
  --cb_type arg         contextual bandit method to use in {ips,dm,dr}
  --eval                Evaluate a policy rather than optimizing.
  --cb_force_legacy     Default to non-adf cb implementation (cb_to_cb_adf)
Contextual Bandit with Action Dependent Features:
  --cb_adf              Do Contextual Bandit learning with multiline action 
                        dependent features.
  --rank_all            Return actions sorted by score order
  --no_predict          Do not do a prediction when training
  --clip_p arg (=0, )   Clipping probability in importance weight. Default: 0.f
                        (no clipping).
  --cb_type arg         contextual bandit method to use in {ips, dm, dr, mtr, 
                        sm}. Default: mtr
CB Distributionally Robust Optimization:
  --cb_dro                     Use DRO for cb learning
  --cb_dro_alpha arg (=0.05, ) Confidence level for cb dro
  --cb_dro_tau arg (=0.999, )  Time constant for count decay for cb dro
  --cb_dro_wmax arg (=inf, )   maximum importance weight for cb_dro
Contextual Bandit Exploration:
  --cb_explore arg        Online explore-exploit for a <k> action contextual 
                          bandit problem
  --first arg             tau-first exploration
  --epsilon arg (=0.05, ) epsilon-greedy exploration
  --bag arg               bagging-based exploration
  --cover arg             Online cover based exploration
  --nounif                do not explore uniformly on zero-probability actions 
                          in cover
  --psi arg (=1, )        disagreement parameter for cover
Contextual Bandit Exploration with ADF (bagging):
  --cb_explore_adf      Online explore-exploit for a contextual bandit problem 
                        with multiline action dependent features
  --epsilon arg         epsilon-greedy exploration
  --bag arg             bagging-based exploration
  --greedify            always update first policy once in bagging
  --first_only          Only explore the first action in a tie-breaking event
Contextual Bandit Exploration with ADF (online cover):
  --cb_explore_adf        Online explore-exploit for a contextual bandit 
                          problem with multiline action dependent features
  --cover arg             Online cover based exploration
  --psi arg (=1, )        disagreement parameter for cover
  --nounif                do not explore uniformly on zero-probability actions 
                          in cover
  --first_only            Only explore the first action in a tie-breaking event
  --cb_type arg           contextual bandit method to use in {ips,dr,mtr}. 
                          Default: mtr
  --epsilon arg (=0.05, ) epsilon-greedy exploration
Contextual Bandit Exploration with ADF (tau-first):
  --cb_explore_adf      Online explore-exploit for a contextual bandit problem 
                        with multiline action dependent features
  --first arg           tau-first exploration
  --epsilon arg         epsilon-greedy exploration
Contextual Bandit Exploration with ADF (greedy):
  --cb_explore_adf      Online explore-exploit for a contextual bandit problem 
                        with multiline action dependent features
  --epsilon arg         epsilon-greedy exploration
  --first_only          Only explore the first action in a tie-breaking event
Contextual Bandit Exploration with ADF (RegCB):
  --cb_explore_adf          Online explore-exploit for a contextual bandit 
                            problem with multiline action dependent features
  --regcb                   RegCB-elim exploration
  --regcbopt                RegCB optimistic exploration
  --mellowness arg (=0.1, ) RegCB mellowness parameter c_0. Default 0.1
  --cb_min_cost arg (=0, )  lower bound on cost
  --cb_max_cost arg (=1, )  upper bound on cost
  --first_only              Only explore the first action in a tie-breaking 
                            event
  --cb_type arg             contextual bandit method to use in {ips,dr,mtr}. 
                            Default: mtr
Contextual Bandit Exploration with ADF (rnd):
  --cb_explore_adf             Online explore-exploit for a contextual bandit 
                               problem with multiline action dependent features
  --epsilon arg                minimum exploration probability
  --rnd arg                    rnd based exploration
  --rnd_alpha arg (=0.1, )     ci width for rnd (bigger => more exploration on 
                               repeating features)
  --rnd_invlambda arg (=0.1, ) covariance regularization strength rnd (bigger 
                               => more exploration on new features)
Contextual Bandit Exploration with ADF (softmax):
  --cb_explore_adf      Online explore-exploit for a contextual bandit problem 
                        with multiline action dependent features
  --epsilon arg         epsilon-greedy exploration
  --softmax             softmax exploration
  --lambda arg (=1, )   parameter for softmax
Contextual Bandit Exploration with ADF (SquareCB):
  --cb_explore_adf              Online explore-exploit for a contextual bandit 
                                problem with multiline action dependent 
                                features
  --squarecb                    SquareCB exploration
  --gamma_scale arg (=10, )     Sets SquareCB greediness parameter to 
                                gamma=[gamma_scale]*[num examples]^1/2
  --gamma_exponent arg (=0.5, ) Exponent on [num examples] in SquareCB 
                                greediness parameter gamma.
  --elim                        Only perform SquareCB exploration over 
                                plausible actions (computed via RegCB strategy)
  --mellowness arg (=0.001, )   Mellowness parameter c_0 for computing 
                                plausible action set. Only used with --elim
  --cb_min_cost arg (=0, )      Lower bound on cost. Only used with --elim
  --cb_max_cost arg (=1, )      Upper bound on cost. Only used with --elim
  --cb_type arg                 contextual bandit method to use in 
                                {ips,dr,mtr}. Default: mtr
Contextual Bandit Exploration with ADF (synthetic cover):
  --cb_explore_adf              Online explore-exploit for a contextual bandit 
                                problem with multiline action dependent 
                                features
  --epsilon arg                 epsilon-greedy exploration
  --synthcover                  use synthetic cover exploration
  --synthcoverpsi arg (=0.1, )  exploration reward bonus
  --synthcoversize arg (=100, ) number of policies in cover
Continuous actions - cb_explore_pdf:
  --cb_explore_pdf        Sample a pdf and pick a continuous valued action
  --epsilon arg (=0.05, ) epsilon-greedy exploration
  --min_value arg (=0, )  min value for continuous range
  --max_value arg (=1, )  max value for continuous range
  --first_only            Use user provided first action or user provided pdf 
                          or uniform random
CB Sample:
  --cb_sample           Sample from CB pdf and swap top action.
Contextual Bandit Options: cb -> cb_adf:
  --cb_to_cbadf arg     Maps cb_adf to cb. Disable with cb_force_legacy.
  --cb arg              Maps cb_adf to cb. Disable with cb_force_legacy.
  --cb_explore arg      Translate cb explore to cb_explore_adf. Disable with 
                        cb_force_legacy.
  --cbify arg           Translate cbify to cb_adf. Disable with 
                        cb_force_legacy.
  --cb_type arg         contextual bandit method to use in {}
  --cb_force_legacy     Default to non-adf cb implementation (cb_algs)
Make Multiclass into Contextual Bandit:
  --cbify arg                  Convert multiclass on <k> classes into a 
                               contextual bandit problem
  --cbify_cs                   Consume cost-sensitive classification examples 
                               instead of multiclass
  --cbify_reg                  Consume regression examples instead of 
                               multiclass and cost sensitive
  --cats arg (=0, )            Continuous action tree with smoothing
  --cb_discrete                Discretizes continuous space and adds cb_explore
                               as option
  --min_value arg              Minimum continuous value
  --max_value arg              Maximum continuous value
  --loss_option arg (=0, )     loss options for regression - 0:squared, 
                               1:absolute, 2:0/1
  --loss_report arg (=0, )     loss report option - 0:normalized, 
                               1:denormalized
  --loss_01_ratio arg (=0.1, ) ratio of zero loss for 0/1 loss
  --loss0 arg (=0, )           loss for correct label
  --loss1 arg (=1, )           loss for incorrect label
Make csoaa_ldf into Contextual Bandit:
  --cbify_ldf           Convert csoaa_ldf into a contextual bandit problem
  --loss0 arg (=0, )    loss for correct label
  --loss1 arg (=1, )    loss for incorrect label
Continuous Action Contextual Bandit using Zeroth-Order Optimization:
  --cbzo                   Solve 1-slot Continuous Action Contextual Bandit 
                           using Zeroth-Order Optimization
  --policy arg (=linear, ) Policy/Model to Learn
  --radius arg (=0.1, )    Exploration Radius
EXPERIMENTAL: Conditional Contextual Bandit Exploration with ADF:
  --ccb_explore_adf     EXPERIMENTAL: Do Conditional Contextual Bandit learning
                        with multiline action dependent features.
  --all_slots_loss      Report average loss from all slots
importance weight classes:
  --classweight arg     importance weight multiplier for class
Confidence:
  --confidence                 Get confidence for binary predictions
  --confidence_after_training  Confidence after training
count_label options:
  --dont_output_best_constant  Don't track the best constant used in the 
                               output.
Cost-sensitive Active Learning:
  --cs_active arg                       Cost-sensitive active learning with <k>
                                        costs
  --simulation                          cost-sensitive active learning 
                                        simulation mode
  --baseline                            cost-sensitive active learning baseline
  --domination arg (=1, )               cost-sensitive active learning use 
                                        domination. Default 1
  --mellowness arg (=0.1, )             mellowness parameter c_0. Default 0.1.
  --range_c arg (=0.5, )                parameter controlling the threshold for
                                        per-label cost uncertainty. Default 
                                        0.5.
  --max_labels arg (=18446744073709551615, )
                                        maximum number of label queries.
  --min_labels arg (=18446744073709551615, )
                                        minimum number of label queries.
  --cost_max arg (=1, )                 cost upper bound. Default 1.
  --cost_min arg (=0, )                 cost lower bound. Default 0.
  --csa_debug                           print debug stuff for cs_active
Cost Sensitive One Against All:
  --csoaa arg           One-against-all multiclass with <k> costs
Cost Sensitive One Against All with Label Dependent Features:
  --csoaa_ldf arg       Use one-against-all multiclass learning with label 
                        dependent features.
  --ldf_override arg    Override singleline or multiline from csoaa_ldf or 
                        wap_ldf, eg if stored in file
  --csoaa_rank          Return actions sorted by score order
  --probabilities       predict probabilities of all classes
Cost Sensitive weighted all-pairs with Label Dependent Features:
  --wap_ldf arg         Use weighted all-pairs multiclass learning with label 
                        dependent features.  Specify singleline or multiline.
Error Correcting Tournament Options:
  --ect arg                Error correcting tournament with <k> labels
  --error arg (=0, )       errors allowed by ECT
  --link arg (=identity, ) Specify the link function: identity, logistic, glf1 
                           or poisson
Explore evaluation:
  --explore_eval        Evaluate explore_eval adf policies
  --multiplier arg      Multiplier used to make all rejection sample 
                        probabilities <= 1
Debug: Metrics:
  --extra_metrics arg   Specify filename to write metrics to. Note: There is no
                        fixed schema.
Follow the Regularized Leader:
  --ftrl                FTRL: Follow the Proximal Regularized Leader
  --coin                Coin betting optimizer
  --pistol              PiSTOL: Parameter-free STOchastic Learning
  --ftrl_alpha arg      Learning rate for FTRL optimization
  --ftrl_beta arg       Learning rate for FTRL optimization
Gradient Descent options:
  --sgd                  use regular stochastic gradient descent update.
  --adaptive             use adaptive, individual learning rates.
  --adax                 use adaptive learning rates with x^2 instead of g^2x^2
  --invariant            use safe/importance aware updates.
  --normalized           use per feature normalized updates
  --sparse_l2 arg (=0, ) use per feature normalized updates
  --l1_state arg (=0, )  use per feature normalized updates
  --l2_state arg (=1, )  use per feature normalized updates
Generate interactions:
  --leave_duplicate_interactions  Don't remove interactions with duplicate 
                                  combinations of namespaces. For ex. this is a
                                  duplicate: '-q ab -q ba' and a lot more in 
                                  '-q ::'.
Continuous actions - convert to pmf:
  --get_pmf             Convert a single multiclass prediction to a pmf
Interact via elementwise multiplication:
  --interact arg        Put weights on feature products from namespaces <n1> 
                        and <n2>
Kernel SVM:
  --ksvm                   kernel svm
  --reprocess arg (=1, )   number of reprocess steps for LASVM
  --pool_greedy            use greedy selection on mini pools
  --para_active            do parallel active learning
  --pool_size arg (=1, )   size of pools for active learning
  --subsample arg (=1, )   number of items to subsample from the pool
  --kernel arg (=linear, ) type of kernel (rbf or linear (default))
  --bandwidth arg (=1, )   bandwidth of rbf kernel
  --degree arg (=2, )      degree of poly kernel
Latent Dirichlet Allocation:
  --lda arg                    Run lda with <int> topics
  --lda_alpha arg (=0.1, )     Prior on sparsity of per-document topic weights
  --lda_rho arg (=0.1, )       Prior on sparsity of topic distributions
  --lda_D arg (=10000, )       Number of documents
  --lda_epsilon arg (=0.001, ) Loop convergence threshold
  --minibatch arg (=1, )       Minibatch size, for LDA
  --math-mode arg (=0, )       Math mode: simd, accuracy, fast-approx
  --metrics                    Compute metrics
Logarithmic Time Multiclass Tree:
  --log_multi arg              Use online tree for multiclass
  --no_progress                disable progressive validation
  --swap_resistance arg (=4, ) higher = more resistance to swap, default=4
Low Rank Quadratics:
  --lrq arg             use low rank quadratic features
  --lrqdropout          use dropout training for low rank quadratic features
Low Rank Quadratics FA:
  --lrqfa arg           use low rank quadratic features with field aware 
                        weights
Marginal options:
  --marginal arg                   substitute marginal label estimates for ids
  --initial_denominator arg (=1, ) initial denominator
  --initial_numerator arg (=0.5, ) initial numerator
  --compete                        enable competition with marginal features
  --update_before_learn            update marginal values before learning
  --unweighted_marginals           ignore importance weights when computing 
                                   marginals
  --decay arg (=0, )               decay multiplier per event (1e-3 for 
                                   example)
Memory Tree:
  --memory_tree arg (=0, )             Make a memory tree with at most <n> 
                                       nodes
  --max_number_of_labels arg (=10, )   max number of unique label
  --leaf_example_multiplier arg (=1, ) multiplier on examples per leaf (default
                                       = log nodes)
  --alpha arg (=0.1, )                 Alpha
  --dream_repeats arg (=1, )           number of dream operations per example 
                                       (default = 1)
  --top_K arg (=1, )                   top K prediction error (default 1)
  --learn_at_leaf                      Enable learning at leaf
  --oas                                use oas at the leaf
  --dream_at_update arg (=0, )         turn on dream operations at reward based
                                       update as well
  --online                             turn on dream operations at reward based
                                       update as well
Multilabel One Against All:
  --multilabel_oaa arg     One-against-all multilabel with <k> labels
  --probabilities          predict probabilities of all classes
  --link arg (=identity, ) Specify the link function: identity, logistic, glf1 
                           or poisson
Multiworld Testing Options:
  --multiworld_test arg Evaluate features as a policies
  --learn arg           Do Contextual Bandit learning on <n> classes.
  --exclude_eval        Discard mwt policy features before learning
Matrix Factorization Reduction:
  --new_mf arg          rank for reduction-based matrix factorization
Neural Network:
  --nn arg              Sigmoidal feedforward network with <k> hidden units
  --inpass              Train or test sigmoidal feedforward network with input 
                        passthrough.
  --multitask           Share hidden layer across all reduced tasks.
  --dropout             Train or test sigmoidal feedforward network using 
                        dropout.
  --meanfield           Train or test sigmoidal feedforward network using mean 
                        field.
Noop Learner:
  --noop                do no learning
One Against All Options:
  --oaa arg             One-against-all multiclass with <k> labels
  --oaa_subsample arg   subsample this number of negative examples when 
                        learning
  --probabilities       predict probabilities of all classes
  --scores              output raw scores per class
Offset tree Options:
  --ot arg              Offset tree with <k> labels
Probabilistic Label Tree :
  --plt arg                Probabilistic Label Tree with <k> labels
  --kary_tree arg (=2, )   use <k>-ary tree
  --threshold arg (=0.5, ) predict labels with conditional marginal probability
                           greater than <thr> threshold
  --top_k arg (=0, )       predict top-<k> labels instead of labels above 
                           threshold
Convert discrete PMF into continuous PDF:
  --pmf_to_pdf arg (=0, ) number of discrete actions <k> for pmf_to_pdf
  --min_value arg         Minimum continuous value
  --max_value arg         Maximum continuous value
  --bandwidth arg         Bandwidth (radius) of randomization around discrete 
                          actions in terms of continuous range. By default will
                          be set to half of the continuous action unit-range 
                          resulting in smoothing that stays inside the action 
                          space unit-range:
                          unit_range = (max_value - min_value)/num-of-actions
                          default bandwidth = unit_range / 2.0
  --first_only            Use user provided first action or user provided pdf 
                          or uniform random
Print psuedolearner:
  --print               print examples
Gradient Descent Matrix Factorization:
  --rank arg            rank for matrix factorization.
  --bfgs                Option not supported by this reduction
  --conjugate_gradient  Option not supported by this reduction
Recall Tree:
  --recall_tree arg       Use online tree for multiclass
  --max_candidates arg    maximum number of labels per leaf in the tree
  --bern_hyper arg (=1, ) recall tree depth penalty
  --max_depth arg         maximum depth of the tree, default log_2 (#classes)
  --node_only             only use node features, not full path features
  --randomized_routing    randomized routing
Experience Replay / replay_b:
  --replay_b arg              use experience replay at a specified level 
                              [b=classification/regression, m=multiclass, 
                              c=cost sensitive] with specified buffer size
  --replay_b_count arg (=1, ) how many times (in expectation) should each 
                              example be played (default: 1 = permuting)
Experience Replay / replay_c:
  --replay_c arg              use experience replay at a specified level 
                              [b=classification/regression, m=multiclass, 
                              c=cost sensitive] with specified buffer size
  --replay_c_count arg (=1, ) how many times (in expectation) should each 
                              example be played (default: 1 = permuting)
Experience Replay / replay_m:
  --replay_m arg              use experience replay at a specified level 
                              [b=classification/regression, m=multiclass, 
                              c=cost sensitive] with specified buffer size
  --replay_m_count arg (=1, ) how many times (in expectation) should each 
                              example be played (default: 1 = permuting)
Continuous actions - sample pdf:
  --sample_pdf          Sample a pdf and pick a continuous valued action
scorer options:
  --link arg (=identity, ) Specify the link function: identity, logistic, glf1 
                           or poisson
Search options:
  --search arg                          Use learning to search, 
                                        argument=maximum action id or 0 for LDF
  --search_task arg                     the search task (use "--search_task 
                                        list" to get a list of available tasks)
  --search_metatask arg                 the search metatask (use 
                                        "--search_metatask list" to get a list 
                                        of available metatasks. Note: a valid 
                                        search_task needs to be supplied in 
                                        addition for this to output.)
  --search_interpolation arg            at what level should interpolation 
                                        happen? [*data|policy]
  --search_rollout arg                  how should rollouts be executed?       
                                            [policy|oracle|*mix_per_state|mix_p
                                        er_roll|none]
  --search_rollin arg                   how should past trajectories be 
                                        generated? [policy|oracle|*mix_per_stat
                                        e|mix_per_roll]
  --search_passes_per_policy arg (=1, ) number of passes per policy (only valid
                                        for search_interpolation=policy)
  --search_beta arg (=0.5, )            interpolation rate for policies (only 
                                        valid for search_interpolation=policy)
  --search_alpha arg (=1e-10, )         annealed beta = 1-(1-alpha)^t (only 
                                        valid for search_interpolation=data)
  --search_total_nb_policies arg        if we are going to train the policies 
                                        through multiple separate calls to vw, 
                                        we need to specify this parameter and 
                                        tell vw how many policies are 
                                        eventually going to be trained
  --search_trained_nb_policies arg      the number of trained policies in a 
                                        file
  --search_allowed_transitions arg      read file of allowed transitions [def: 
                                        all transitions are allowed]
  --search_subsample_time arg           instead of training at all timesteps, 
                                        use a subset. if value in (0,1), train 
                                        on a random v%. if v>=1, train on 
                                        precisely v steps per example, if 
                                        v<=-1, use active learning
  --search_neighbor_features arg        copy features from neighboring lines. 
                                        argument looks like: '-1:a,+2' meaning 
                                        copy previous line namespace a and next
                                        next line from namespace _unnamed_, 
                                        where ',' separates them
  --search_rollout_num_steps arg        how many calls of "loss" before we stop
                                        really predicting on rollouts and 
                                        switch to oracle (default means 
                                        "infinite")
  --search_history_length arg (=1, )    some tasks allow you to specify how 
                                        much history their depend on; specify 
                                        that here
  --search_no_caching                   turn off the built-in caching ability 
                                        (makes things slower, but technically 
                                        more safe)
  --search_xv                           train two separate policies, 
                                        alternating prediction/learning
  --search_perturb_oracle arg (=0, )    perturb the oracle on rollin with this 
                                        probability
  --search_linear_ordering              insist on generating examples in linear
                                        order (def: hoopla permutation)
  --search_active_verify arg            verify that active learning is doing 
                                        the right thing (arg = multiplier, 
                                        should be = cost_range * range_c)
  --search_save_every_k_runs arg        save model every k runs
Network sending:
  --sendto arg          send examples to <host>
Slates:
  --slates              EXPERIMENTAL
Stagewise polynomial options:
  --stage_poly                use stagewise polynomial feature learning
  --sched_exponent arg (=1, ) exponent controlling quantity of included 
                              features
  --batch_sz arg (=1000, )    multiplier on batch size before including more 
                              features
  --batch_sz_no_doubling      batch_sz does not double
Stochastic Variance Reduced Gradient:
  --svrg                  Streaming Stochastic Variance Reduced Gradient
  --stage_size arg (=1, ) Number of passes per SVRG stage
Top K:
  --top arg             top k recommendation
Make Multiclass into Warm-starting Contextual Bandit:
  --warm_cb arg                        Convert multiclass on <k> classes into a
                                       contextual bandit problem
  --warm_cb_cs                         consume cost-sensitive classification 
                                       examples instead of multiclass
  --loss0 arg (=0, )                   loss for correct label
  --loss1 arg (=1, )                   loss for incorrect label
  --warm_start arg (=0, )              number of training examples for warm 
                                       start phase
  --epsilon arg                        epsilon-greedy exploration
  --interaction arg (=4294967295, )    number of examples for the interactive 
                                       contextual bandit learning phase
  --warm_start_update                  indicator of warm start updates
  --interaction_update                 indicator of interaction updates
  --corrupt_type_warm_start arg (=1, ) type of label corruption in the warm 
                                       start phase (1: uniformly at random, 2: 
                                       circular, 3: replacing with overwriting 
                                       label)
  --corrupt_prob_warm_start arg (=0, ) probability of label corruption in the 
                                       warm start phase
  --choices_lambda arg (=1, )          the number of candidate lambdas to 
                                       aggregate (lambda is the importance 
                                       weight parameter between the two 
                                       sources)
  --lambda_scheme arg (=1, )           The scheme for generating candidate 
                                       lambda set (1: center lambda=0.5, 2: 
                                       center lambda=0.5, min lambda=0, max 
                                       lambda=1, 3: center lambda=epsilon/(1+ep
                                       silon), 4: center lambda=epsilon/(1+epsi
                                       lon), min lambda=0, max lambda=1); the 
                                       rest of candidate lambda values are 
                                       generated using a doubling scheme
  --overwrite_label arg (=1, )         the label used by type 3 corruptions 
                                       (overwriting)
  --sim_bandit                         simulate contextual bandit updates on 
                                       warm start examples
