dnn_init(): num_layers: 3 hidden_layer_size: 20 num_inputs: 20 contraction: 1, activation: relu, optimizer: SGD
using cache_file = synthetic_data.txt.cache
ignoring text input in favor of cache input
num sources = 1
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
Enabled learners: dnn, scorer-identity, count_label
Input label = SIMPLE
Output pred = SCALAR
average  since         example        example        current        current  current
loss     last          counter         weight          label        predict features
4.131231 4.131231         1000         1000.0         9.8946         9.0305       11
2.166660 0.202089         2000         2000.0         3.6186         3.5312       11
1.502101 0.172983         3000         3000.0         0.3482         0.4230       11
1.144400 0.071298         4000         4000.0        -5.0053        -4.9415       11
0.945793 0.151366         5000         5000.0         3.3605         3.2900       11
0.800069 0.071445         6000         6000.0        -0.4698        -0.5409       11
0.692102 0.044305         7000         7000.0         4.2052         4.1014       11
0.610606 0.040132         8000         8000.0         2.5320         2.6948       11
0.564770 0.198081         9000         9000.0         7.3241         7.4298       11
0.724833 0.724833        10000        10000.0         9.8946        10.0471       11 h
0.661298 0.025380        11000        11000.0         3.6186         3.5425       11 h
0.607963 0.020796        12000        12000.0         0.3482         0.4220       11 h
0.563988 0.035896        13000        13000.0        -5.0053        -5.1299       11 h
0.528673 0.069252        14000        14000.0         3.3605         3.4177       11 h
0.494685 0.018555        15000        15000.0        -0.4698        -0.5240       11 h
0.465192 0.022520        16000        16000.0         4.2052         4.1319       11 h
0.439212 0.023300        17000        17000.0         2.5320         2.5331       11 h
0.417876 0.054974        18000        18000.0         7.3241         7.2969       11 h
0.396621 0.017257        19000        19000.0         9.8946         9.8842       11 h
0.377451 0.012876        20000        20000.0         3.6186         3.5411       11 h
0.360378 0.018614        21000        21000.0         0.3482         0.3919       11 h
0.345285 0.028068        22000        22000.0        -5.0053        -5.0480       11 h
0.332024 0.040036        23000        23000.0         3.3605         3.4409       11 h
0.318886 0.016469        24000        24000.0        -0.4698        -0.4851       11 h
0.306840 0.017526        25000        25000.0         4.2052         4.1123       11 h
0.295811 0.019897        26000        26000.0         2.5320         2.5097       11 h
0.286642 0.048069        27000        27000.0         7.3241         7.2980       11 h
0.276848 0.014589        28000        28000.0         9.8946         9.8326       11 h
0.267659 0.010134        29000        29000.0         3.6186         3.5582       11 h
0.259248 0.015089        30000        30000.0         0.3482         0.3258       11 h
0.251802 0.028212        31000        31000.0        -5.0053        -5.0510       11 h
0.245071 0.036235        32000        32000.0         3.3605         3.4654       11 h
0.238124 0.015639        33000        33000.0        -0.4698        -0.4995       11 h
0.231589 0.015745        34000        34000.0         4.2052         4.1086       11 h
0.225513 0.018783        35000        35000.0         2.5320         2.5433       11 h
0.220094 0.030286        36000        36000.0         7.3241         7.3164       11 h

finished run
number of examples per pass = 9000
passes used = 4
weighted example sum = 36000.000000
weighted label sum = 0.000000
average loss = 0.000000 h
best constant = 0.000000
total feature number = 395984
